- These numerical experiments, written in Python 3.6, are used to support the results in the paper: 
"Dong Quan Vu, Patrick Loiseau, and Alonso Silva. Combinatorial Bandits for Sequential Learning in Colonel Blotto Games. Under review in 58th Conference on Decision and Control 2019."
- Contact authors at dongquan[dot]math[at]gmail[dot]com


Black box optimization for searching an improved exploration distribution can be found in the "Blackbox Exloration folder". It outputs the file explore.csv.
The EdgeCB algorithm with exploration distribution taking input from explore.csv; outputs to output.csv.


Abstracts: The Colonel Blotto game is a renowned resource allocation problem with a long-standing literature in game theory (almost 100 years). However, its scope of application is still restricted by the lack of studies on the incompleteinformation situations where a learning model is needed. In this work, we propose and study a regret-minimization model where a learner repeatedly plays the Colonel Blotto game against several adversaries. At each time step, the learner distributes her budget of resources on a fixed number of battlefields to maximize the aggregate value of battlefields she wins; each battlefield being won if there is no adversary that has higher
allocation. We focus on the setting with minimal feedback—the bandit feedback. We first show that it can be modeled as a Path Planning problem. It is then possible to use the classical COMBAND algorithm to guarantee a sub-linear regret in terms of time horizon, but this entails two fundamental challenges: (i) the computation is inefficient due to the huge size of the action set, and (ii) the standard exploration distribution leads to a loose guarantee in practice. To address the first, we construct a modified algorithm that can be efficiently implemented by applying a dynamic programming technique called weight pushing; for the second, we propose methods optimizing the exploration distribution to improve the regret bound. Finally, we implement our proposed algorithm and perform numerical experiments that show the regret improvement in practice.

Description of COMBINATORIAL BANDIT MODEL OFLEARNING IN COLONEL BLOTTO GAMES:
We consider a sequential learning problem that involves alearner, A adversaries, n battlefields and a time horizon T (n≥2 and T>0 are known  by the learner, A≥1). Each battlefield i∈{1,...,n} has a fixed value b_i>0 (hidden from the learner) and we assume normalized values, that is sum of b_i, i∈{1,...,n} is 1. At  each  time  step t ∈ {1,...,T},  the  learner  faces  a  decision problem  of  distributing m troops (m≥1 is fixed) towards the battlefields while the adversaries simultaneously allocate theirs. The  learner’s allocations have to satisfy the budget constraint, that is she chooses a strategy p^t in the action set S:={(p_1,...,p_n):∑p_i=m}. For any i∈{1,...,n}, the element p^t_i of strategy prepresents the quantity of troops she allocates to battlefield i. At the end of time t, the learner suffers a loss L(p^t) equal to the sum of values of battlefields that she loses, i.e., where there is at least one adversary having strictly higher local allocation than her. Without loss of generality, in case where the learner and α adversaries (α ≥ 1) have tie allocations which are the highest in battlefield i, we assume that the learner loses [α/(α + 1)]b_i in this battlefield. When t ends, the learner observes the scalar number L(p^t) but she does not know which battlefield she lost (or won) or the strategies that adversaries used (the bandit feedback). With the assumption on the battlefields values b_i, i∈{1,...,n}, this incurred loss is bounded, i.e., L(p^t)≤1, ∀p^t, ∀t. The cumulative loss is computed up to the time horizon T and the learner’s objective is to minimize her expected regret.
